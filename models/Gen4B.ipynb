{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ceb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('..')\n",
    "from data.load_data import *\n",
    "from processing.utils import *\n",
    "from NotezartTransformer import NotezartTransformer\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "seed = 2022\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "checkpoint_path = Path('resource/gen4/v2').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6586fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file as an array of events\n",
    "def read_midi(midi_path):\n",
    "    note_items, tempo_items = read_items(midi_path)\n",
    "    note_items = quantize_items(note_items)\n",
    "    max_time = note_items[-1].end\n",
    "    chord_items = extract_chords(note_items)\n",
    "    items = chord_items + tempo_items + note_items\n",
    "    groups = group_items(items, max_time)\n",
    "    events = item2event(groups)\n",
    "    return np.array(events, dtype=object)\n",
    "\n",
    "\n",
    "# read in a series of midi files as a list of sequence of events\n",
    "def transform_midi(midi_paths):\n",
    "    # extract events\n",
    "    events = []\n",
    "    all_events = []\n",
    "    for path in midi_paths:\n",
    "        try:\n",
    "            midi = read_midi(path)\n",
    "            events.append(np.asarray([e.to_key() for e in midi]))\n",
    "            all_events.append(midi)\n",
    "        except:\n",
    "            print(f\"Failed: {path}\")\n",
    "    return all_events, np.asarray(events, dtype=object)\n",
    "\n",
    "\n",
    "def build_lookup(midi_paths, dictionary_path):\n",
    "    all_events, events = transform_midi(midi_paths=midi_paths)\n",
    "\n",
    "    unique_events = np.unique([e for s in events for e in s])\n",
    "    event2word = dict(zip(unique_events, list(range(0, len(unique_events)))))\n",
    "    word2event = {i: e for e, i in event2word.items()}\n",
    "\n",
    "    with open(dictionary_path, 'wb') as handle:\n",
    "        pickle.dump([event2word, word2event], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ed2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(training_set_path, model, all_events):\n",
    "    training_data = model.prepare_data(all_events)\n",
    "\n",
    "    with open(training_set_path, 'wb') as handle:\n",
    "        pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def train_model(dataset_name, pre_load=False):\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    training_set_path = f\"{checkpoint_path}/data/training_set_{dataset_name}.pkl\"\n",
    "    dictionary_path = f\"{checkpoint_path}/dictionary/dictionary_{dataset_name}.pkl\"\n",
    "    midi_paths = get_all_files(dataset_name=dataset_name)\n",
    "\n",
    "    if not pre_load:\n",
    "        all_events = build_lookup(midi_paths=midi_paths, dictionary_path=dictionary_path)\n",
    "\n",
    "    model = NotezartTransformer(checkpoint=checkpoint_path, dataset_name=dataset_name, is_training=True)\n",
    "\n",
    "    if pre_load:\n",
    "        training_data = pickle.load(open(training_set_path, 'rb'))\n",
    "    else:\n",
    "        training_data = get_data(training_set_path, model, all_events)\n",
    "\n",
    "    model.load_model()\n",
    "\n",
    "    output_model_folder = f\"{checkpoint_path}/model\"  # your decision\n",
    "    model.finetune(epochs=100, training_data=training_data, output_checkpoint_folder=output_model_folder)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16802cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, dataset_name):\n",
    "    model = NotezartTransformer(checkpoint=checkpoint_path, dataset_name=dataset_name, is_training=False)\n",
    "    model.load_model(existing_model=model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374c5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Failed: C:/One/CMU/DeepLearning/data/adl-piano-midi/Classical/Classical/Ludwig van Beethoven\\Moonlight Sonata 1st movement.mid\n",
      "Failed: C:/One/CMU/DeepLearning/data/adl-piano-midi/Classical/Classical/Ludwig van Beethoven\\Moonlight Sonata 3rd Movement.mid\n",
      "Failed: C:/One/CMU/DeepLearning/data/adl-piano-midi/Classical/Classical/Ludwig van Beethoven\\Sonata for Piano with Horn, Op 17.mid\n",
      ">>> Epoch: 0, Step: 0, Loss: 7.80376, Time: 4.39\n",
      ">>> Epoch: 0, Step: 10, Loss: 3.99328, Time: 10.99\n",
      ">>> Epoch: 0, Step: 20, Loss: 3.48362, Time: 17.68\n",
      ">>> Epoch: 0, Step: 30, Loss: 3.87957, Time: 24.42\n",
      ">>> Epoch: 0, Step: 40, Loss: 2.97054, Time: 31.01\n",
      ">>> Epoch: 1, Step: 50, Loss: 2.22277, Time: 39.19\n",
      ">>> Epoch: 1, Step: 60, Loss: 2.73815, Time: 45.85\n",
      ">>> Epoch: 1, Step: 70, Loss: 2.24368, Time: 52.41\n",
      ">>> Epoch: 1, Step: 80, Loss: 2.74717, Time: 59.10\n",
      ">>> Epoch: 1, Step: 90, Loss: 2.33678, Time: 65.64\n",
      ">>> Epoch: 2, Step: 100, Loss: 1.97689, Time: 73.81\n",
      ">>> Epoch: 2, Step: 110, Loss: 2.32962, Time: 80.82\n",
      ">>> Epoch: 2, Step: 120, Loss: 1.94532, Time: 87.95\n",
      ">>> Epoch: 2, Step: 130, Loss: 2.45992, Time: 94.45\n",
      ">>> Epoch: 2, Step: 140, Loss: 2.13728, Time: 100.91\n",
      ">>> Epoch: 3, Step: 150, Loss: 1.85612, Time: 108.92\n",
      ">>> Epoch: 3, Step: 160, Loss: 2.09320, Time: 115.47\n",
      ">>> Epoch: 3, Step: 170, Loss: 1.74863, Time: 121.21\n",
      ">>> Epoch: 3, Step: 180, Loss: 2.10561, Time: 127.02\n",
      ">>> Epoch: 3, Step: 190, Loss: 1.80215, Time: 132.75\n",
      ">>> Epoch: 4, Step: 200, Loss: 1.52524, Time: 139.96\n",
      ">>> Epoch: 4, Step: 210, Loss: 1.76218, Time: 145.86\n",
      ">>> Epoch: 4, Step: 220, Loss: 1.48572, Time: 151.59\n",
      ">>> Epoch: 4, Step: 230, Loss: 1.85189, Time: 157.11\n",
      ">>> Epoch: 4, Step: 240, Loss: 1.66468, Time: 162.71\n",
      "WARNING:tensorflow:From c:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1052: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      ">>> Epoch: 5, Step: 250, Loss: 1.46096, Time: 169.89\n",
      ">>> Epoch: 5, Step: 260, Loss: 1.68660, Time: 175.73\n",
      ">>> Epoch: 5, Step: 270, Loss: 1.43945, Time: 181.42\n",
      ">>> Epoch: 5, Step: 280, Loss: 1.74606, Time: 186.97\n",
      ">>> Epoch: 5, Step: 290, Loss: 1.57907, Time: 192.59\n",
      ">>> Epoch: 6, Step: 300, Loss: 1.40869, Time: 199.83\n",
      ">>> Epoch: 6, Step: 310, Loss: 1.60702, Time: 205.56\n",
      ">>> Epoch: 6, Step: 320, Loss: 1.40101, Time: 211.13\n",
      ">>> Epoch: 6, Step: 330, Loss: 1.67042, Time: 216.68\n",
      ">>> Epoch: 6, Step: 340, Loss: 1.51875, Time: 222.32\n",
      ">>> Epoch: 7, Step: 350, Loss: 1.36863, Time: 229.31\n",
      ">>> Epoch: 7, Step: 360, Loss: 1.55231, Time: 234.88\n",
      ">>> Epoch: 7, Step: 370, Loss: 1.36082, Time: 240.54\n",
      ">>> Epoch: 7, Step: 380, Loss: 1.64204, Time: 246.19\n",
      ">>> Epoch: 7, Step: 390, Loss: 1.47240, Time: 251.73\n",
      ">>> Epoch: 8, Step: 400, Loss: 1.33102, Time: 258.66\n",
      ">>> Epoch: 8, Step: 410, Loss: 1.46778, Time: 264.27\n",
      ">>> Epoch: 8, Step: 420, Loss: 1.31722, Time: 269.85\n",
      ">>> Epoch: 8, Step: 430, Loss: 1.60237, Time: 275.36\n",
      ">>> Epoch: 8, Step: 440, Loss: 1.41147, Time: 280.89\n",
      ">>> Epoch: 9, Step: 450, Loss: 1.21978, Time: 287.89\n",
      ">>> Epoch: 9, Step: 460, Loss: 1.33988, Time: 293.45\n",
      ">>> Epoch: 9, Step: 470, Loss: 1.22567, Time: 299.21\n",
      ">>> Epoch: 9, Step: 480, Loss: 1.47165, Time: 305.60\n",
      ">>> Epoch: 9, Step: 490, Loss: 1.33635, Time: 312.08\n",
      ">>> Epoch: 10, Step: 500, Loss: 1.14832, Time: 319.99\n",
      ">>> Epoch: 10, Step: 510, Loss: 1.25333, Time: 326.40\n",
      ">>> Epoch: 10, Step: 520, Loss: 1.16077, Time: 332.92\n",
      ">>> Epoch: 10, Step: 530, Loss: 1.37529, Time: 339.49\n",
      ">>> Epoch: 10, Step: 540, Loss: 1.28250, Time: 346.10\n",
      ">>> Epoch: 11, Step: 550, Loss: 1.10077, Time: 354.11\n",
      ">>> Epoch: 11, Step: 560, Loss: 1.15498, Time: 361.01\n",
      ">>> Epoch: 11, Step: 570, Loss: 1.10184, Time: 367.44\n",
      ">>> Epoch: 11, Step: 580, Loss: 1.27218, Time: 373.82\n",
      ">>> Epoch: 11, Step: 590, Loss: 1.22196, Time: 380.26\n",
      ">>> Epoch: 12, Step: 600, Loss: 1.03593, Time: 388.04\n",
      ">>> Epoch: 12, Step: 610, Loss: 1.07637, Time: 394.57\n",
      ">>> Epoch: 12, Step: 620, Loss: 1.05027, Time: 401.06\n",
      ">>> Epoch: 12, Step: 630, Loss: 1.20468, Time: 407.48\n",
      ">>> Epoch: 12, Step: 640, Loss: 1.12023, Time: 413.90\n",
      ">>> Epoch: 13, Step: 650, Loss: 0.98618, Time: 421.77\n",
      ">>> Epoch: 13, Step: 660, Loss: 0.98624, Time: 428.43\n",
      ">>> Epoch: 13, Step: 670, Loss: 1.00044, Time: 434.99\n",
      ">>> Epoch: 13, Step: 680, Loss: 1.14036, Time: 441.43\n",
      ">>> Epoch: 13, Step: 690, Loss: 1.05822, Time: 447.83\n",
      ">>> Epoch: 14, Step: 700, Loss: 0.92064, Time: 455.57\n",
      ">>> Epoch: 14, Step: 710, Loss: 0.91400, Time: 461.94\n",
      ">>> Epoch: 14, Step: 720, Loss: 0.93745, Time: 468.36\n",
      ">>> Epoch: 14, Step: 730, Loss: 1.10801, Time: 474.75\n",
      ">>> Epoch: 14, Step: 740, Loss: 1.00397, Time: 481.35\n",
      ">>> Epoch: 15, Step: 750, Loss: 0.86876, Time: 489.25\n",
      ">>> Epoch: 15, Step: 760, Loss: 0.89203, Time: 496.02\n",
      ">>> Epoch: 15, Step: 770, Loss: 0.90237, Time: 502.74\n",
      ">>> Epoch: 15, Step: 780, Loss: 1.02343, Time: 509.15\n",
      ">>> Epoch: 15, Step: 790, Loss: 0.94895, Time: 515.66\n",
      ">>> Epoch: 16, Step: 800, Loss: 0.80868, Time: 523.46\n",
      ">>> Epoch: 16, Step: 810, Loss: 0.82743, Time: 530.12\n",
      ">>> Epoch: 16, Step: 820, Loss: 0.85382, Time: 536.67\n",
      ">>> Epoch: 16, Step: 830, Loss: 0.97942, Time: 543.15\n",
      ">>> Epoch: 16, Step: 840, Loss: 0.90524, Time: 549.58\n",
      ">>> Epoch: 17, Step: 850, Loss: 0.75554, Time: 557.51\n",
      ">>> Epoch: 17, Step: 860, Loss: 0.76659, Time: 564.18\n",
      ">>> Epoch: 17, Step: 870, Loss: 0.81082, Time: 570.62\n",
      ">>> Epoch: 17, Step: 880, Loss: 0.91847, Time: 577.69\n",
      ">>> Epoch: 17, Step: 890, Loss: 0.85793, Time: 584.21\n",
      ">>> Epoch: 18, Step: 900, Loss: 0.69294, Time: 592.27\n",
      ">>> Epoch: 18, Step: 910, Loss: 0.73621, Time: 598.78\n",
      ">>> Epoch: 18, Step: 920, Loss: 0.75120, Time: 605.25\n",
      ">>> Epoch: 18, Step: 930, Loss: 0.86033, Time: 611.99\n",
      ">>> Epoch: 18, Step: 940, Loss: 0.81608, Time: 618.80\n",
      ">>> Epoch: 19, Step: 950, Loss: 0.65903, Time: 626.71\n",
      ">>> Epoch: 19, Step: 960, Loss: 0.68267, Time: 633.75\n",
      ">>> Epoch: 19, Step: 970, Loss: 0.69444, Time: 640.51\n",
      ">>> Epoch: 19, Step: 980, Loss: 0.85907, Time: 646.92\n",
      ">>> Epoch: 19, Step: 990, Loss: 0.78693, Time: 653.33\n",
      ">>> Epoch: 20, Step: 1000, Loss: 0.62224, Time: 661.25\n",
      ">>> Epoch: 20, Step: 1010, Loss: 0.63204, Time: 667.84\n",
      ">>> Epoch: 20, Step: 1020, Loss: 0.63053, Time: 674.18\n",
      ">>> Epoch: 20, Step: 1030, Loss: 0.80202, Time: 680.52\n",
      ">>> Epoch: 20, Step: 1040, Loss: 0.75073, Time: 686.86\n",
      ">>> Epoch: 21, Step: 1050, Loss: 0.58097, Time: 694.57\n",
      ">>> Epoch: 21, Step: 1060, Loss: 0.59660, Time: 700.94\n",
      ">>> Epoch: 21, Step: 1070, Loss: 0.59174, Time: 708.10\n",
      ">>> Epoch: 21, Step: 1080, Loss: 0.72391, Time: 714.74\n",
      ">>> Epoch: 21, Step: 1090, Loss: 0.67428, Time: 721.37\n",
      ">>> Epoch: 22, Step: 1100, Loss: 0.56823, Time: 729.34\n",
      ">>> Epoch: 22, Step: 1110, Loss: 0.57498, Time: 735.77\n",
      ">>> Epoch: 22, Step: 1120, Loss: 0.53745, Time: 742.16\n",
      ">>> Epoch: 22, Step: 1130, Loss: 0.69364, Time: 748.70\n",
      ">>> Epoch: 22, Step: 1140, Loss: 0.63133, Time: 755.15\n",
      ">>> Epoch: 23, Step: 1150, Loss: 0.48045, Time: 762.86\n",
      ">>> Epoch: 23, Step: 1160, Loss: 0.51403, Time: 769.20\n",
      ">>> Epoch: 23, Step: 1170, Loss: 0.46538, Time: 775.54\n",
      ">>> Epoch: 23, Step: 1180, Loss: 0.65257, Time: 781.90\n",
      ">>> Epoch: 23, Step: 1190, Loss: 0.58239, Time: 788.48\n",
      ">>> Epoch: 24, Step: 1200, Loss: 0.44421, Time: 796.47\n",
      ">>> Epoch: 24, Step: 1210, Loss: 0.43811, Time: 803.05\n",
      ">>> Epoch: 24, Step: 1220, Loss: 0.42299, Time: 809.49\n",
      ">>> Epoch: 24, Step: 1230, Loss: 0.59327, Time: 815.82\n",
      ">>> Epoch: 24, Step: 1240, Loss: 0.51980, Time: 822.15\n",
      ">>> Epoch: 25, Step: 1250, Loss: 0.38676, Time: 829.84\n",
      ">>> Epoch: 25, Step: 1260, Loss: 0.42369, Time: 836.17\n",
      ">>> Epoch: 25, Step: 1270, Loss: 0.41102, Time: 842.50\n",
      ">>> Epoch: 25, Step: 1280, Loss: 0.55720, Time: 848.83\n",
      ">>> Epoch: 25, Step: 1290, Loss: 0.47071, Time: 855.18\n",
      ">>> Epoch: 26, Step: 1300, Loss: 0.36118, Time: 863.07\n",
      ">>> Epoch: 26, Step: 1310, Loss: 0.38426, Time: 869.71\n",
      ">>> Epoch: 26, Step: 1320, Loss: 0.34236, Time: 876.12\n",
      ">>> Epoch: 26, Step: 1330, Loss: 0.48720, Time: 882.49\n",
      ">>> Epoch: 26, Step: 1340, Loss: 0.43975, Time: 888.84\n",
      ">>> Epoch: 27, Step: 1350, Loss: 0.33191, Time: 896.57\n",
      ">>> Epoch: 27, Step: 1360, Loss: 0.32831, Time: 902.90\n",
      ">>> Epoch: 27, Step: 1370, Loss: 0.28255, Time: 909.54\n",
      ">>> Epoch: 27, Step: 1380, Loss: 0.46416, Time: 916.59\n",
      ">>> Epoch: 27, Step: 1390, Loss: 0.40414, Time: 923.64\n",
      ">>> Epoch: 28, Step: 1400, Loss: 0.28047, Time: 931.77\n",
      ">>> Epoch: 28, Step: 1410, Loss: 0.31359, Time: 938.79\n",
      ">>> Epoch: 28, Step: 1420, Loss: 0.26492, Time: 945.57\n",
      ">>> Epoch: 28, Step: 1430, Loss: 0.41628, Time: 952.29\n",
      ">>> Epoch: 28, Step: 1440, Loss: 0.35542, Time: 959.30\n",
      ">>> Epoch: 29, Step: 1450, Loss: 0.22192, Time: 967.29\n",
      ">>> Epoch: 29, Step: 1460, Loss: 0.28055, Time: 973.79\n",
      ">>> Epoch: 29, Step: 1470, Loss: 0.24495, Time: 980.21\n",
      ">>> Epoch: 29, Step: 1480, Loss: 0.41727, Time: 986.68\n",
      ">>> Epoch: 29, Step: 1490, Loss: 0.29906, Time: 993.12\n",
      ">>> Epoch: 30, Step: 1500, Loss: 0.22866, Time: 1001.08\n",
      ">>> Epoch: 30, Step: 1510, Loss: 0.26082, Time: 1007.62\n",
      ">>> Epoch: 30, Step: 1520, Loss: 0.23680, Time: 1014.43\n",
      ">>> Epoch: 30, Step: 1530, Loss: 0.33463, Time: 1020.90\n",
      ">>> Epoch: 30, Step: 1540, Loss: 0.29219, Time: 1027.25\n",
      ">>> Epoch: 31, Step: 1550, Loss: 0.23697, Time: 1035.27\n",
      ">>> Epoch: 31, Step: 1560, Loss: 0.26239, Time: 1041.65\n",
      ">>> Epoch: 31, Step: 1570, Loss: 0.18466, Time: 1048.15\n",
      ">>> Epoch: 31, Step: 1580, Loss: 0.32002, Time: 1054.90\n",
      ">>> Epoch: 31, Step: 1590, Loss: 0.25877, Time: 1061.53\n",
      ">>> Epoch: 32, Step: 1600, Loss: 0.16726, Time: 1069.46\n",
      ">>> Epoch: 32, Step: 1610, Loss: 0.20111, Time: 1075.83\n",
      ">>> Epoch: 32, Step: 1620, Loss: 0.18545, Time: 1082.21\n",
      ">>> Epoch: 32, Step: 1630, Loss: 0.28778, Time: 1088.74\n",
      ">>> Epoch: 32, Step: 1640, Loss: 0.26592, Time: 1095.15\n",
      ">>> Epoch: 33, Step: 1650, Loss: 0.18277, Time: 1102.92\n",
      ">>> Epoch: 33, Step: 1660, Loss: 0.20208, Time: 1109.31\n",
      ">>> Epoch: 33, Step: 1670, Loss: 0.15890, Time: 1115.77\n",
      ">>> Epoch: 33, Step: 1680, Loss: 0.26774, Time: 1122.26\n",
      ">>> Epoch: 33, Step: 1690, Loss: 0.19436, Time: 1128.65\n",
      ">>> Epoch: 34, Step: 1700, Loss: 0.13751, Time: 1136.38\n",
      ">>> Epoch: 34, Step: 1710, Loss: 0.19640, Time: 1142.74\n",
      ">>> Epoch: 34, Step: 1720, Loss: 0.12733, Time: 1149.36\n",
      ">>> Epoch: 34, Step: 1730, Loss: 0.21367, Time: 1155.83\n",
      ">>> Epoch: 34, Step: 1740, Loss: 0.18753, Time: 1162.20\n",
      ">>> Epoch: 35, Step: 1750, Loss: 0.13254, Time: 1170.11\n",
      ">>> Epoch: 35, Step: 1760, Loss: 0.16146, Time: 1176.64\n",
      ">>> Epoch: 35, Step: 1770, Loss: 0.11587, Time: 1183.12\n",
      ">>> Epoch: 35, Step: 1780, Loss: 0.18956, Time: 1189.59\n",
      ">>> Epoch: 35, Step: 1790, Loss: 0.17339, Time: 1196.27\n",
      ">>> Epoch: 36, Step: 1800, Loss: 0.09989, Time: 1204.02\n",
      ">>> Epoch: 36, Step: 1810, Loss: 0.13284, Time: 1210.59\n",
      ">>> Epoch: 36, Step: 1820, Loss: 0.10811, Time: 1217.19\n",
      ">>> Epoch: 36, Step: 1830, Loss: 0.18049, Time: 1223.70\n",
      ">>> Epoch: 36, Step: 1840, Loss: 0.16632, Time: 1230.20\n",
      ">>> Epoch: 37, Step: 1850, Loss: 0.11428, Time: 1237.94\n",
      ">>> Epoch: 37, Step: 1860, Loss: 0.12619, Time: 1244.55\n",
      ">>> Epoch: 37, Step: 1870, Loss: 0.08050, Time: 1250.96\n",
      ">>> Epoch: 37, Step: 1880, Loss: 0.19022, Time: 1257.50\n",
      ">>> Epoch: 37, Step: 1890, Loss: 0.16481, Time: 1263.98\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\"ADL\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\One\\CMU\\DeepLearning\\note-zart\\models\\resource\\gen4\\v2/model/model-021-0.647\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model(f\"{checkpoint_path}/model/model-025-0.647\", \"ADL\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c0e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\One\\CMU\\DeepLearning\\note-zart\\models\\resource\\gen4\\v2/output/sample_1_0.midi\n",
      "C:\\One\\CMU\\DeepLearning\\note-zart\\models\\resource\\gen4\\v2/output/sample_1_1.midi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_target_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                   \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcheckpoint_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/output/sample_1_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43ma\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.midi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\One\\CMU\\DeepLearning\\note-zart\\models\\NotezartTransformer.py:202\u001B[0m, in \u001B[0;36mNotezartTransformer.generate\u001B[1;34m(self, n_target_bar, temperature, topk, output_path, prompt)\u001B[0m\n\u001B[0;32m    200\u001B[0m     feed_dict[m] \u001B[38;5;241m=\u001B[39m m_np\n\u001B[0;32m    201\u001B[0m \u001B[38;5;66;03m# model (prediction)\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m _logits, _new_mem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_mem\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeed_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;66;03m# sampling\u001B[39;00m\n\u001B[0;32m    204\u001B[0m _logit \u001B[38;5;241m=\u001B[39m _logits[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:967\u001B[0m, in \u001B[0;36mBaseSession.run\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    964\u001B[0m run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 967\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions_ptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    969\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[0;32m    970\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1190\u001B[0m, in \u001B[0;36mBaseSession._run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001B[39;00m\n\u001B[0;32m   1189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m final_fetches \u001B[38;5;129;01mor\u001B[39;00m final_targets \u001B[38;5;129;01mor\u001B[39;00m (handle \u001B[38;5;129;01mand\u001B[39;00m feed_dict_tensor):\n\u001B[1;32m-> 1190\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_targets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_fetches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mfeed_dict_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1193\u001B[0m   results \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1370\u001B[0m, in \u001B[0;36mBaseSession._do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1367\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001B[0;32m   1369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1370\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_run_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1371\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1373\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1377\u001B[0m, in \u001B[0;36mBaseSession._do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_do_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m   1376\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1377\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1378\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOpError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1379\u001B[0m     message \u001B[38;5;241m=\u001B[39m compat\u001B[38;5;241m.\u001B[39mas_text(e\u001B[38;5;241m.\u001B[39mmessage)\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1360\u001B[0m, in \u001B[0;36mBaseSession._do_run.<locals>._run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1357\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_fn\u001B[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001B[0;32m   1358\u001B[0m   \u001B[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001B[39;00m\n\u001B[0;32m   1359\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extend_graph()\n\u001B[1;32m-> 1360\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_tf_sessionrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\one\\cmu\\deeplearning\\note-zart\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1453\u001B[0m, in \u001B[0;36mBaseSession._call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1451\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_tf_sessionrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, options, feed_dict, fetch_list, target_list,\n\u001B[0;32m   1452\u001B[0m                         run_metadata):\n\u001B[1;32m-> 1453\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRun_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1454\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for a in range(10):\n",
    "    model.generate(n_target_bar=20, temperature=2, topk=4,\n",
    "                   output_path=f\"{checkpoint_path}/output/sample_1_{a}.midi\", prompt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}