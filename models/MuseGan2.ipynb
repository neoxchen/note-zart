{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuseGan2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import codebase from drive"
      ],
      "metadata": {
        "id": "D38MNtI6tIed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zQRbr6pdt1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"https://drive.google.com/drive/folders/1bAZFATe5ialGaIlN00LEn9hQUvFXjUpo?usp=sharing\")"
      ],
      "metadata": {
        "id": "o36kZ1u-t3pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from musegan.config import LOGLEVEL, LOG_FORMAT\n",
        "from musegan.data import load_data, get_dataset, get_samples\n",
        "from musegan.metrics import get_save_metric_ops\n",
        "from musegan.model import Model\n",
        "from musegan.utils import make_sure_path_exists, load_yaml\n",
        "from musegan.utils import backup_src, update_not_none, setup_loggers"
      ],
      "metadata": {
        "id": "V9CvE96PuedE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import dependencies"
      ],
      "metadata": {
        "id": "xpQOfS8nummK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAp1BJh0sgSM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "from pprint import pformat\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import tensorflow as tf\n",
        "LOGGER = logging.getLogger(\"musegan.train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse arguments"
      ],
      "metadata": {
        "id": "B1OGd5r6tNRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arguments():\n",
        "    \"\"\"Parse and return the command line arguments.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--exp_dir', help=\"Directory to save all the results.\")\n",
        "    parser.add_argument('--params', help=\"Path to the model parameter file.\")\n",
        "    parser.add_argument('--config', help=\"Path to the configuration file.\")\n",
        "    parser.add_argument('--gpu', '--gpu_device_num', type=str, default=\"0\",\n",
        "                        help=\"The GPU device number to use.\")\n",
        "    parser.add_argument('--n_jobs', type=int,\n",
        "                        help=\"Number of parallel calls to use for input \"\n",
        "                             \"pipeline. Set to 1 to disable multiprocessing.\")\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "0FLDiKCuszZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "f5izFR4ftWHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dirs(config):\n",
        "    \"\"\"Setup an experiment directory structure and update the `params`\n",
        "    dictionary with the directory paths.\"\"\"\n",
        "    # Get experiment directory structure\n",
        "    config['exp_dir'] = os.path.realpath(config['exp_dir'])\n",
        "    config['src_dir'] = os.path.join(config['exp_dir'], 'src')\n",
        "    config['eval_dir'] = os.path.join(config['exp_dir'], 'eval')\n",
        "    config['model_dir'] = os.path.join(config['exp_dir'], 'model')\n",
        "    config['sample_dir'] = os.path.join(config['exp_dir'], 'samples')\n",
        "    config['log_dir'] = os.path.join(config['exp_dir'], 'logs', 'train')\n",
        "\n",
        "    # Make sure directories exist\n",
        "    for key in ('log_dir', 'model_dir', 'sample_dir', 'src_dir'):\n",
        "        make_sure_path_exists(config[key])\n",
        "\n",
        "def setup():\n",
        "    \"\"\"Parse command line arguments, load model parameters, load configurations,\n",
        "    setup environment and setup loggers.\"\"\"\n",
        "    # Parse the command line arguments\n",
        "    args = parse_arguments()\n",
        "\n",
        "    # Load parameters\n",
        "    params = load_yaml(args.params)\n",
        "    if params.get('is_accompaniment') and params.get('condition_track_idx') is None:\n",
        "        raise TypeError(\"`condition_track_idx` cannot be None type in \"\n",
        "                        \"accompaniment mode.\")\n",
        "\n",
        "    # Load configurations\n",
        "    config = load_yaml(args.config)\n",
        "    update_not_none(config, vars(args))\n",
        "\n",
        "    # Set unspecified schedule steps to default values\n",
        "    for target in (config['learning_rate_schedule'], config['slope_schedule']):\n",
        "        if target['start'] is None:\n",
        "            target['start'] = 0\n",
        "        if target['end'] is None:\n",
        "            target['end'] = config['steps']\n",
        "\n",
        "    # Setup experiment directories and update them to configuration dictionary\n",
        "    setup_dirs(config)\n",
        "\n",
        "    # Setup loggers\n",
        "    del logging.getLogger('tensorflow').handlers[0]\n",
        "    setup_loggers(config['log_dir'])\n",
        "\n",
        "    # Setup GPUs\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = config['gpu']\n",
        "\n",
        "    # Backup source code\n",
        "    backup_src(config['src_dir'])\n",
        "\n",
        "    return params, config\n"
      ],
      "metadata": {
        "id": "YRTXltwxtRD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "afHh9NaFtbIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training_data(params, config):\n",
        "    \"\"\"Load and return the training data.\"\"\"\n",
        "    # Load data\n",
        "    if params['is_conditional']:\n",
        "        raise ValueError(\"Not supported yet.\")\n",
        "    else:\n",
        "        labels = None\n",
        "    LOGGER.info(\"Loading training data.\")\n",
        "    data = load_data(config['data_source'], config['data_filename'])\n",
        "    LOGGER.info(\"Training data size: %d\", len(data))\n",
        "\n",
        "    # Build dataset\n",
        "    LOGGER.info(\"Building dataset.\")\n",
        "    dataset = get_dataset(\n",
        "        data, labels, config['batch_size'], params['data_shape'],\n",
        "        config['use_random_transpose'], config['n_jobs'])\n",
        "\n",
        "    # Create iterator\n",
        "    if params['is_conditional']:\n",
        "        train_x, train_y = dataset.make_one_shot_iterator().get_next()\n",
        "    else:\n",
        "        train_x, train_y = dataset.make_one_shot_iterator().get_next(), None\n",
        "\n",
        "    return train_x, train_y"
      ],
      "metadata": {
        "id": "JK7vXW-ds5Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create samples from data"
      ],
      "metadata": {
        "id": "N892pCrctjAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_or_create_samples(params, config):\n",
        "    \"\"\"Load or create the samples used as the sampler inputs.\"\"\"\n",
        "    # Load sample_z\n",
        "    LOGGER.info(\"Loading sample_z.\")\n",
        "    sample_z_path = os.path.join(config['model_dir'], 'sample_z.npy')\n",
        "    if os.path.exists(sample_z_path):\n",
        "        sample_z = np.load(sample_z_path)\n",
        "        if sample_z.shape[1] != params['latent_dim']:\n",
        "            LOGGER.info(\"Loaded sample_z has wrong shape\")\n",
        "            resample = True\n",
        "        else:\n",
        "            resample = False\n",
        "    else:\n",
        "        LOGGER.info(\"File for sample_z not found\")\n",
        "        resample = True\n",
        "\n",
        "    # Draw new sample_z\n",
        "    if resample:\n",
        "        LOGGER.info(\"Drawing new sample_z.\")\n",
        "        sample_z = scipy.stats.truncnorm.rvs(\n",
        "            -2, 2, size=(np.prod(config['sample_grid']), params['latent_dim']))\n",
        "        make_sure_path_exists(config['model_dir'])\n",
        "        np.save(sample_z_path, sample_z)\n",
        "\n",
        "    if params.get('is_accompaniment'):\n",
        "        # Load sample_x\n",
        "        LOGGER.info(\"Loading sample_x.\")\n",
        "        sample_x_path = os.path.join(config['model_dir'], 'sample_x.npy')\n",
        "        if os.path.exists(sample_x_path):\n",
        "            sample_x = np.load(sample_x_path)\n",
        "            if sample_x.shape[1:] != params['data_shape']:\n",
        "                LOGGER.info(\"Loaded sample_x has wrong shape\")\n",
        "                resample = True\n",
        "            else:\n",
        "                resample = False\n",
        "        else:\n",
        "            LOGGER.info(\"File for sample_x not found\")\n",
        "            resample = True\n",
        "\n",
        "        # Draw new sample_x\n",
        "        if resample:\n",
        "            LOGGER.info(\"Drawing new sample_x.\")\n",
        "            data = load_data(config['data_source'], config['data_filename'])\n",
        "            sample_x = get_samples(\n",
        "                np.prod(config['sample_grid']), data,\n",
        "                use_random_transpose = config['use_random_transpose'])\n",
        "            make_sure_path_exists(config['model_dir'])\n",
        "            np.save(sample_x_path, sample_x)\n",
        "    else:\n",
        "        sample_x = None\n",
        "\n",
        "    return sample_x, None, sample_z"
      ],
      "metadata": {
        "id": "q4lsBuA5tg6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main - Training  "
      ],
      "metadata": {
        "id": "wy-gpD6-tm03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    # Setup\n",
        "    logging.basicConfig(level=LOGLEVEL, format=LOG_FORMAT)\n",
        "    params, config = setup()\n",
        "    LOGGER.info(\"Using parameters:\\n%s\", pformat(params))\n",
        "    LOGGER.info(\"Using configurations:\\n%s\", pformat(config))\n",
        "\n",
        "    # ================================== Data ==================================\n",
        "    # Load training data\n",
        "    train_x, _ = load_training_data(params, config)\n",
        "\n",
        "    # ================================= Model ==================================\n",
        "    # Build model\n",
        "    model = Model(params)\n",
        "    if params.get('is_accompaniment'):\n",
        "        train_c = tf.expand_dims(\n",
        "            train_x[..., params['condition_track_idx']], -1)\n",
        "        train_nodes = model(\n",
        "            x=train_x, c=train_c, mode='train', params=params, config=config)\n",
        "    else:\n",
        "        train_nodes = model(\n",
        "            x=train_x, mode='train', params=params, config=config)\n",
        "\n",
        "    # Log number of parameters in the model\n",
        "    def get_n_params(var_list):\n",
        "        \"\"\"Return the number of variables in a variable list.\"\"\"\n",
        "        return int(np.sum([np.product(\n",
        "            [x.value for x in var.get_shape()]) for var in var_list]))\n",
        "\n",
        "    LOGGER.info(\"Number of trainable parameters in {}: {:,}\".format(\n",
        "        model.name, get_n_params(tf.trainable_variables(model.name))))\n",
        "    for component in model.components:\n",
        "        LOGGER.info(\"Number of trainable parameters in {}: {:,}\".format(\n",
        "            component.name, get_n_params(tf.trainable_variables(\n",
        "                model.name + '/' + component.name))))\n",
        "\n",
        "    # ================================ Sampler =================================\n",
        "    if config['save_samples_steps'] > 0:\n",
        "        # Get sampler inputs\n",
        "        sample_x, sample_y, sample_z = load_or_create_samples(params, config)\n",
        "\n",
        "        # Create sampler configurations\n",
        "        sampler_config = {\n",
        "            'result_dir': config['sample_dir'],\n",
        "            'suffix': tf.as_string(train_nodes['gen_step']),\n",
        "            'image_grid': config['sample_grid'],\n",
        "            'colormap': np.array(config['colormap']).T,\n",
        "            'midi': config['midi'],\n",
        "            'collect_save_arrays_op': config['save_array_samples'],\n",
        "            'collect_save_images_op': config['save_image_samples'],\n",
        "            'collect_save_pianorolls_op': config['save_pianoroll_samples']}\n",
        "\n",
        "        # Get prediction nodes\n",
        "        placeholder_z = tf.placeholder(tf.float32, shape=sample_z.shape)\n",
        "        placeholder_y = None\n",
        "        if params.get('is_accompaniment'):\n",
        "            c_shape = np.append(sample_x.shape[:-1], 1)\n",
        "            placeholder_c = tf.placeholder(tf.float32, shape=c_shape)\n",
        "            predict_nodes = model(\n",
        "                z=placeholder_z, y=placeholder_y, c=placeholder_c,\n",
        "                mode='predict', params=params, config=sampler_config)\n",
        "        else:\n",
        "            predict_nodes = model(\n",
        "                z=placeholder_z, y=placeholder_y, mode='predict', params=params,\n",
        "                config=sampler_config)\n",
        "\n",
        "        # Get sampler op\n",
        "        sampler_op = tf.group([\n",
        "            predict_nodes[key] for key in (\n",
        "                'save_arrays_op', 'save_images_op', 'save_pianorolls_op')\n",
        "            if key in predict_nodes])\n",
        "        sampler_op_no_pianoroll = tf.group([\n",
        "            predict_nodes[key] for key in ('save_arrays_op', 'save_images_op')\n",
        "            if key in predict_nodes])\n",
        "\n",
        "    # ================================ Metrics =================================\n",
        "    if config['evaluate_steps'] > 0:\n",
        "        binarized = tf.round(.5 * (predict_nodes['fake_x'] + 1.))\n",
        "        save_metric_ops = get_save_metric_ops(\n",
        "            binarized, params['beat_resolution'], train_nodes['gen_step'],\n",
        "            config['eval_dir'])\n",
        "        save_metrics_op = tf.group(save_metric_ops)\n",
        "\n",
        "    # ========================== Training Preparation ==========================\n",
        "    # Get tensorflow session config\n",
        "    tf_config = tf.ConfigProto()\n",
        "    tf_config.gpu_options.allow_growth = True\n",
        "\n",
        "    # Training hooks\n",
        "    global_step = tf.train.get_global_step()\n",
        "    steps_per_iter = config['n_dis_updates_per_gen_update'] + 1\n",
        "    hooks = [tf.train.NanTensorHook(train_nodes['loss'])]\n",
        "\n",
        "    # Tensor logger\n",
        "    tensor_logger = {\n",
        "        'step': train_nodes['gen_step'],\n",
        "        'gen_loss': train_nodes['gen_loss'],\n",
        "        'dis_loss': train_nodes['dis_loss']}\n",
        "    step_logger = open(os.path.join(config['log_dir'], 'step.log'), 'w')\n",
        "\n",
        "    # ======================= Monitored Training Session =======================\n",
        "    LOGGER.info(\"Training start.\")\n",
        "    with tf.train.MonitoredTrainingSession(\n",
        "        save_checkpoint_steps=config['save_checkpoint_steps'] * steps_per_iter,\n",
        "        save_summaries_steps=config['save_summaries_steps'] * steps_per_iter,\n",
        "        checkpoint_dir=config['model_dir'], log_step_count_steps=0,\n",
        "        hooks=hooks, config=tf_config) as sess:\n",
        "\n",
        "        # Get global step value\n",
        "        step = tf.train.global_step(sess, global_step)\n",
        "        if step == 0:\n",
        "            step_logger.write('# step, gen_loss, dis_loss\\n')\n",
        "\n",
        "        # ============================== Training ==============================\n",
        "        if step >= config['steps']:\n",
        "            LOGGER.info(\"Global step has already exceeded total steps.\")\n",
        "            step_logger.close()\n",
        "            return\n",
        "\n",
        "        # Training iteration\n",
        "        while step < config['steps']:\n",
        "\n",
        "            # Train the discriminator\n",
        "            if step < 10:\n",
        "                n_dis_updates = 10 * config['n_dis_updates_per_gen_update']\n",
        "            else:\n",
        "                n_dis_updates = config['n_dis_updates_per_gen_update']\n",
        "            for _ in range(n_dis_updates):\n",
        "                sess.run(train_nodes['train_ops']['dis'])\n",
        "\n",
        "            # Train the generator\n",
        "            log_loss_steps = config['log_loss_steps'] or 100\n",
        "            if (step + 1) % log_loss_steps == 0:\n",
        "                step, _, tensor_logger_values = sess.run([\n",
        "                    train_nodes['gen_step'], train_nodes['train_ops']['gen'],\n",
        "                    tensor_logger])\n",
        "                # Logger\n",
        "                if config['log_loss_steps'] > 0:\n",
        "                    LOGGER.info(\"step={}, {}\".format(\n",
        "                        tensor_logger_values['step'], ', '.join([\n",
        "                            '{}={: 8.4E}'.format(key, value)\n",
        "                            for key, value in tensor_logger_values.items()\n",
        "                            if key != 'step'])))\n",
        "                step_logger.write(\"{}, {: 10.6E}, {: 10.6E}\\n\".format(\n",
        "                    tensor_logger_values['step'],\n",
        "                    tensor_logger_values['gen_loss'],\n",
        "                    tensor_logger_values['dis_loss']))\n",
        "            else:\n",
        "                step, _ = sess.run([\n",
        "                    train_nodes['gen_step'], train_nodes['train_ops']['gen']])\n",
        "\n",
        "            # Run sampler\n",
        "            if ((config['save_samples_steps'] > 0)\n",
        "                    and (step % config['save_samples_steps'] == 0)):\n",
        "                LOGGER.info(\"Running sampler\")\n",
        "                feed_dict_sampler = {placeholder_z: sample_z}\n",
        "                if params.get('is_accompaniment'):\n",
        "                    feed_dict_sampler[placeholder_c] = np.expand_dims(\n",
        "                        sample_x[..., params['condition_track_idx']], -1)\n",
        "                if step < 3000:\n",
        "                    sess.run(\n",
        "                        sampler_op_no_pianoroll, feed_dict=feed_dict_sampler)\n",
        "                else:\n",
        "                    sess.run(sampler_op, feed_dict=feed_dict_sampler)\n",
        "\n",
        "            # Run evaluation\n",
        "            if ((config['evaluate_steps'] > 0)\n",
        "                    and (step % config['evaluate_steps'] == 0)):\n",
        "                LOGGER.info(\"Running evaluation\")\n",
        "                feed_dict_evaluation = {\n",
        "                    placeholder_z: scipy.stats.truncnorm.rvs(-2, 2, size=(\n",
        "                        np.prod(config['sample_grid']), params['latent_dim']))}\n",
        "                if params.get('is_accompaniment'):\n",
        "                    feed_dict_evaluation[placeholder_c] = np.expand_dims(\n",
        "                        sample_x[..., params['condition_track_idx']], -1)\n",
        "                sess.run(save_metrics_op, feed_dict=feed_dict_evaluation)\n",
        "\n",
        "            # Stop training if stopping criterion suggests\n",
        "            if sess.should_stop():\n",
        "                break\n",
        "\n",
        "    LOGGER.info(\"Training end\")\n",
        "    step_logger.close()\n"
      ],
      "metadata": {
        "id": "uZKMcwxPs9No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fNLMPu9FtEzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}